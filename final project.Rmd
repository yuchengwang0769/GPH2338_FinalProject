---
title: "Final Project for GPH 2338 Machine Learning in Publich Health"
author: "Olivia Chien and Yucheng Wang"
date: "2024-03-25"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
library(leaps)
library(glmnet)
library(caret)
```

## Data download

Since ICPSR restricts redistribute the data, a proper way is to download the data to a local directory, for example, the project directory. Only codes and outputs should be push to Github. The data should be kept in the local. To download, use the email and password to login ICPSR.

```{r, eval=FALSE}
# download data from ICPSR to local directory
library(icpsrdata)
icpsr_download(file_id = 37516)
```

```{r}
# import downloaded data
load(file='icpsr_data/37516-0001-Data.rda')
raw <- da37516.0001
```

## Data cleaning

```{r}
# Drop missing data, sample size=4,090
data <- raw[complete.cases(raw[ , c("SMOKER_CATEGORY","ALCOHOL","TRACT_BELOW_POVERTY","TRACT_UNINSURED_OVER_65","TRACT_OWNER_OVER_30","TRACT_UNINSURED_UNDER_18","COUNTY_VIOLENT_CRIME")]),]

# Keep only needed columns
data <- data %>% 
  dplyr::select(starts_with("TRACT_")|starts_with("COUNTY_")|starts_with("AIR_")|all_of(c("AGE","GENDER","RACETHNICITY","EDUC4","MARITAL","EMPLOY","INCOME","REGION9","METRO","HOUSING","HOME_TYPE","SMOKER_CATEGORY","ALCOHOL","PHYSICIANS_TG_DOC","WALK_SCORE")))


# Impute those missing air quality data with median
data$missing.airdata <- factor(ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), 1, 0),levels = c(1,0),labels = c("Yes","No"))
data$AIR_QUALITY_GOOD_DAYS <- ifelse(is.na(data$AIR_QUALITY_GOOD_DAYS), median(data$AIR_QUALITY_GOOD_DAYS,na.rm = TRUE), data$AIR_QUALITY_GOOD_DAYS)
data$AIR_QUALITY_MEDIAN_AQI <- ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), median(data$AIR_QUALITY_MEDIAN_AQI,na.rm = TRUE), data$AIR_QUALITY_MEDIAN_AQI)
data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT <- ifelse(is.na(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT), median(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT,na.rm = TRUE), data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT)
data$AIR_QUALITY_DAYS_PM2_5<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM2_5), median(data$AIR_QUALITY_DAYS_PM2_5,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM2_5)
data$AIR_QUALITY_DAYS_PM10<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM10), median(data$AIR_QUALITY_DAYS_PM10,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM10)
```


```{r}
# Add dummy variables for categorical variables
for (x in c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOL","METRO","HOME_TYPE","SMOKER_CATEGORY","REGION9")){
  lbls <- levels(data[[x]])
  levels(data[[x]]) <- sub("^\\([0-9]+\\) +(.+$)", "\\1", lbls)
}

dummy <- fastDummies::dummy_cols(data, remove_first_dummy=TRUE)

# Drop original categorical variables to make the final data
data <- dummy[ , !(names(dummy) %in% c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOL","METRO","HOME_TYPE",
                                       "SMOKER_CATEGORY","REGION9", "missing.airdata"))]

# Revise variable names
names(data) <- make.names(names(data), unique=TRUE)

rm(dummy, da37516.0001)

```


```{r}
# Draw training sample, 50% training, 50% test data
set.seed(1)
train <- sample(1:nrow(data),nrow(data)/2)
```


## Linear Regression

```{r}
# Overall linear regression that include all variables
origin.lm <- lm(WALK_SCORE ~ ., data=data[train,])
pred.test.lm <- predict(origin.lm, newdata = data[-train,])
mean((pred.test.lm-data[-train,"WALK_SCORE"])^2)

# Function to calculate test error with coefficient
func.coeftoerror <- function(coef){
  test.data <- data[-train,]
  te_x <- test.data %>% dplyr::select(names(coef)[-1])
  te_pred <- cbind(1, as.matrix(te_x)) %*% coef
  te_error <- mean((te_pred - test.data$WALK_SCORE)^2)
  selected.var <- paste(names(coef)[-1], collapse = ",")
  data.frame(test.error = te_error, n.var=length(names(coef))-1,selected.var=selected.var)
}

#SUBSET SELECTION
#identifying a subset of the predictors we believe to be related to response, fit a model using least squares on the reduced set of variables
#not using best subset due to number of variables, possible models to fit would be huge (2^107)
#perhaps hybrid approach of forward and backward stepwise, as it mimics best subset while retaining computational advantages of forward/backward stepwise
#but am running into issues with multicollinearlity? 

#best subset
#bs.fit <- regsubsets(WALK_SCORE~., data)
#too long

#forward, backward, hybrid
set.seed(1)

fwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="forward", nvmax = 50)
bwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="backward", nvmax = 50)
hybrid.fit <- regsubsets(WALK_SCORE~., data=data[train,],method="seqrep", nvmax = 50)

fwd.summary <- summary(fwd.fit)
bwd.summary<-summary(bwd.fit)
hybrid.summary<-summary(hybrid.fit)

# USE BIC as criteria when choosing the best model for each method
best.ind.fwd <- which.min(fwd.summary$bic)
best.ind.bwd <- which.min(bwd.summary$bic)
best.ind.hybrid <- which.min(hybrid.summary$bic)

best_coef.fwd <- coef(fwd.fit, best.ind.fwd)
best_coef.bwd <- coef(bwd.fit, best.ind.bwd)
best_coef.hybrid <- coef(hybrid.fit, best.ind.hybrid)

rbind(func.coeftoerror(best_coef.fwd),func.coeftoerror(best_coef.bwd),func.coeftoerror(best_coef.hybrid))

# selection methods may be inappropriate for our analysis because of multicollienarity in our data, we also used shrinkage methods

```

```{r}
#SHRINKAGE methods, fitting a model involving all predictors, estimated coefficients are shrunken towards zero relative to the least squares estimate, can also perform variable selection. 
#ridge:L2, lasso: L1

#ridge or LASSO? 
#i dont think we need ridge since it doesn't shrink variables to 0? but maybe we could talk about the variables that are close to 0? 

test <- (-train)
x <- as.matrix(subset(data, select = -WALK_SCORE))
y <- data$WALK_SCORE
y.test <- y[test]

#ridge
#cross validation to choose tuning parameter lambda
set.seed(1)
ridge.cv.out <- cv.glmnet(x[train,], y[train], alpha=0, nfolds = 5)
ridge.lam <- ridge.cv.out$lambda.min

#ridge regression with the selected lambda
ridge.pred <- predict(ridge.cv.out, newx=x[test,],s="lambda.min")
mean((ridge.pred-y.test)^2)

ridge.coef.val <- predict(ridge.cv.out, type="coefficients", s="lambda.min")

#since ridge does not shrink towards 0 or peanlize as heavily as lasso, just looking at the variables that seem more sig
ridge.coef.ind <- which(abs(ridge.coef.val) >= 10) 
ridge.coef.names <- ridge.coef.val@Dimnames[[1]][ridge.coef.ind]
ridge.coef.tab <- cbind(ridge.coef.names, ridge.coef.val[ridge.coef.ind])

#LASSO
set.seed(1)

#cross validation to choose tuning parameter lambda
lasso.cv.out <- cv.glmnet(x[train,], y[train], alpha=1, nfolds = 5)
lasso.lam <- lasso.cv.out$lambda.min 

lasso.lr <- glmnet(x[train,], y[train], alpha=1, lambda=lasso.lam, thresh=1e-12)
lasso.pred <- predict(lasso.lr, newx=x[test,])
mean((lasso.pred-y.test)^2)

lasso.coef <- predict(lasso.cv.out, type="coefficients", s=lasso.lam)       
lasso.coef

```



## KNN for regression

```{r}

fit_std <- preProcess(data[train,], method = c("center","scale"))
train_std <- predict(fit_std, newdata = data[train,])
train_std$WALK_SCORE <- data[train,"WALK_SCORE"]
test_std <- predict(fit_std, newdata = data[-train,])
test_std$WALK_SCORE <- data[-train,"WALK_SCORE"]
set.seed(1)
fold_ind <- sample(1:5, nrow(data[-train,]), replace = TRUE)

knn_by_k <-  function(k) {
    cv.error <- mean(sapply(1:5, function(j){
    fit <- knnreg(WALK_SCORE ~ ., data = train_std[fold_ind != j, ], k = k)
    pred <- predict(fit, newdata = train_std[fold_ind == j, ])
    mean((train_std$WALK_SCORE[fold_ind == j] - pred)^2)
    }))
    data.frame(k=k, cv.error=cv.error)
  }
knn_cv <- do.call(rbind,lapply(1:100,knn_by_k))
plot(knn_cv$k,knn_cv$cv.error)
best_k <- knn_cv$k[knn_cv$cv.error==min(knn_cv$cv.error)]

knn.final <- knnreg(WALK_SCORE ~ ., data = train_std, k = best_k)
knn.pred <- predict(knn.final, newdata=test_std)
mean((knn.pred-data[-train,"WALK_SCORE"])^2)

```


## Random Forest
```{r}

rf <- randomForest(WALK_SCORE ~ ., data = data[train,])
yhat.rf <- predict(rf, newdata = data[-train,])
mean((yhat.rf-data[-train,"WALK_SCORE"])^2)

varImpPlot(rf)
```

## Boosting

```{r}
boost <- gbm(WALK_SCORE ~ .,data = data[train,], distribution = "gaussian", n.trees = 30000, interaction.depth = 1, cv.folds = 5, shrinkage = 0.01)
best_n_tress <- which.min(boost$cv.error)

yhat.boost <- predict(boost, newdata = data[-train,], n.trees = best_n_tress)
mean((yhat.boost - data[-train,"WALK_SCORE"])^2)
```

