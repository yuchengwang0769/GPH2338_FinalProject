---
title: "Final Project for GPH 2338 Machine Learning in Publich Health"
author: "Olivia Chien and Yucheng Wang"
date: "2024-03-25"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
library(leaps)
library(glmnet)
library(caret)
```

## Data download

Since ICPSR restricts redistribute the data, a proper way is to download the data to a local directory, for example, the project directory. Only codes and outputs should be push to Github. The data should be kept in the local. To download, use the email and password to login ICPSR.

```{r, eval=FALSE}
# download data from ICPSR to local directory
library(icpsrdata)
icpsr_download(file_id = 37516)
```

```{r}
# import downloaded data
load(file='icpsr_data/37516-0001-Data.rda')
raw <- da37516.0001
```

## Data cleaning

```{r}
# Drop missing data, sample size=4,090
data <- raw[complete.cases(raw[ , c("SMOKER_CATEGORY","ALCOHOLFREQUENCY","TRACT_BELOW_POVERTY","TRACT_UNINSURED_OVER_65","TRACT_OWNER_OVER_30","TRACT_UNINSURED_UNDER_18","COUNTY_VIOLENT_CRIME")]),]

# Keep only needed columns
data <- data %>% 
  dplyr::select(starts_with("TRACT_")|starts_with("COUNTY_")|starts_with("AIR_")|all_of(c("AGE","GENDER","RACETHNICITY","EDUC4","MARITAL","EMPLOY","INCOME","REGION9","METRO","HOUSING","HOME_TYPE","SMOKER_CATEGORY","ALCOHOLFREQUENCY","PHYSICIANS_TG_DOC","WALK_SCORE")))


# Impute those missing air quality data with median
data$missing.airdata <- factor(ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), 1, 0),levels = c(1,0),labels = c("Yes","No"))
data$AIR_QUALITY_GOOD_DAYS <- ifelse(is.na(data$AIR_QUALITY_GOOD_DAYS), median(data$AIR_QUALITY_GOOD_DAYS,na.rm = TRUE), data$AIR_QUALITY_GOOD_DAYS)
data$AIR_QUALITY_MEDIAN_AQI <- ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), median(data$AIR_QUALITY_MEDIAN_AQI,na.rm = TRUE), data$AIR_QUALITY_MEDIAN_AQI)
data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT <- ifelse(is.na(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT), median(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT,na.rm = TRUE), data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT)
data$AIR_QUALITY_DAYS_PM2_5<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM2_5), median(data$AIR_QUALITY_DAYS_PM2_5,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM2_5)
data$AIR_QUALITY_DAYS_PM10<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM10), median(data$AIR_QUALITY_DAYS_PM10,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM10)
```


```{r}
# Add dummy variables
for (x in c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOLFREQUENCY","METRO","HOME_TYPE","SMOKER_CATEGORY","REGION9", "missing.airdata")){
  lbls <- levels(data[[x]])
  levels(data[[x]]) <- sub("^\\([0-9]+\\) +(.+$)", "\\1", lbls)
}

dummy <- fastDummies::dummy_cols(data, remove_first_dummy=TRUE)

# Drop original categorical variables to make the final data

data <- dummy[ , !(names(dummy) %in% c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOLFREQUENCY","METRO","HOME_TYPE",
                                       "SMOKER_CATEGORY","REGION9", "missing.airdata"))]

names(data) <- make.names(names(data), unique=TRUE)

rm(dummy, da37516.0001)

```

```{r}
colnames(data) <- gsub("[-\\ - \\ \\,]", "_", gsub("[,\\$]", "", gsub("\\+", "_or_more", gsub("___", "_", colnames(data)))))

data1 <- subset(data,select=-GENDER_Female)
                              
```


```{r}
set.seed(1)
train <- sample(1:nrow(data),nrow(data)/2)
```


## Linear Regression

```{r}
#SUBSET SELECTION
#identifying a subset of the predictors we believe to be related to response, fit a model using least squares on the reduced set of variables
#not using best subset due to number of variables, possible models to fit would be huge (2^107)
#perhaps hybrid approach of forward and backward stepwise, as it mimics best subset while retaining computational advantages of forward/backward stepwise
#but am running into issues with multicollinearlity? 

#best subset
#bs.fit <- regsubsets(WALK_SCORE~., data)
#too long

#forward, backward, hybrid
set.seed(1)

fwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="forward")
bwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="backward")
hybrid.fit <- regsubsets(WALK_SCORE~., data=data[train,],method="seqrep")

fwd.summary <- summary(fwd.fit)
bwd.summary<-summary(bwd.fit)
hybrid.summary<-summary(hybrid.fit)

#lets use cp, i used adj r2 and all the methods gave me only one variable
best.ind.fwd <- which.min(fwd.summary$adjr2)
coef(fwd.fit, best.ind.fwd)
best.ind.fwd <- which.min(fwd.summary$cp) 
coef(fwd.fit, best.ind.fwd)
best.ind.bwd <- which.min(bwd.summary$cp)
coef(bwd.fit, best.ind.bwd)
best.ind.hybrid <- which.min(hybrid.summary$cp)
coef(hybrid.fit, best.ind.hybrid)

#the thing is we can say we didn't use selection methods because of multicollienarity in our data, so we used shrinkage methods

#testing
#finding pairs with high correlation
# data1 <- data
# data1.numeric <- model.matrix(~ . - 1, data = data1)
# var.cor <- cor(data1.numeric)
# which(abs(var.cor) >= 0.95&var.cor!= 1, arr.ind = TRUE)
# which(abs(var.cor) >= 0.90&var.cor!=1, arr.ind = TRUE)
# >=0.95: TRACT_RENTER_OCCUPIED_OF_OCCUPID, TRACT_OWNER_OCCUPIED_OF_OCCUPIED, GENDER_Female, GENDER_Male
#either way i think we should remove GENDER_Female or GENDER_Male
```

```{r}
#SHRINKAGE methods, fitting a model involving all predictors, estimated coefficients are shrunken towards zero relative to the least squares estimate, can also perform variable selection. 
#ridge:L2, lasso: L1

#ridge or LASSO? 
#i dont think we need ridge since it doesn't shrink variables to 0? but maybe we could talk about the variables that are close to 0? 

test <- (-train)
x <- model.matrix(WALK_SCORE~., data)[,-47]
y <- data$WALK_SCORE
y.test <- y[test]

#ridge
#cross validation to choose tuning parameter lambda
set.seed(1)
ridge.cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(ridge.cv.out)
ridge.lam <- ridge.cv.out$lambda.min

ridge.lr <- glmnet(x[train,], y[train], alpha=0, lambda=ridge.lam, thresh=1e-12)
ridge.pred <- predict(ridge.lr, newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.out <- glmnet(x, y, alpha=0)
ridge.coef <- predict(ridge.out, type="coefficients", s=ridge.lam)

ridge.coef.ind <- which(abs(ridge.coef.val) >= 1) #since ridge does not shrink towards 0 or peanlize as heavily as lasso, just looking at the variables that seem more sig
ridge.coef.names <- colnames(x)[ridge.coef.ind]
ridge.coef.tab <- cbind(ridge.coef.names, ridge.coef.val[ridge.coef.ind])


#LASSO
set.seed(1)
#cross vadliation to choose tuning parameter lambda
lasso.cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(lasso.cv.out)
lasso.lam <- lasso.cv.out$lambda.min 

lasso.lr <- glmnet(x[train,], y[train], alpha=1, lambda=lasso.lam, thresh=1e-12)
lasso.pred <- predict(lasso.lr, newx=x[test,])
mean((lasso.pred-y.test)^2)

lasso.out <- glmnet(x, y, alpha=1)
lasso.coef <- predict(lasso.out, type="coefficients", s=lasso.lam)       
lasso.coef
lasso.coef[lasso.coef!=0] #56 variables chosen

```



## KNN for regression

```{r}
std <- preProcess(data[train,], method = c("center","scale"))
train_std <- predict(std, newdata = data[train,])
test_std <- predict(std, newdata = data[-train,])

```


## Random Forest
```{r}
rf <- randomForest(WALK_SCORE ~ ., data = data[train,])
yhat.rf <- predict(rf, newdata = data[-train,])
mean((yhat.rf-data[-train,"WALK_SCORE"])^2)

varImpPlot(rf)
```

## Boosting

```{r}
boost <- gbm(WALK_SCORE ~ .,data = data[train,], distribution = "gaussian", n.trees = 30000, interaction.depth = 1, cv.folds = 5, shrinkage = 0.01)
best_n_tress <- which.min(boost$cv.error)

yhat.boost <- predict(boost, newdata = data[-train,], n.trees = best_n_tress)
mean((yhat.boost - data[-train,"WALK_SCORE"])^2)
```

