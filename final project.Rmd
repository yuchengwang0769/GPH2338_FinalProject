---
title: "Final Project for GPH 2338 Machine Learning in Publich Health"
author: "Olivia Chien and Yucheng Wang"
date: "2024-03-25"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
library(leaps)
library(glmnet)
library(caret)
```

## Data download

Since ICPSR restricts redistribute the data, a proper way is to download the data to a local directory, for example, the project directory. Only codes and outputs should be push to Github. The data should be kept in the local. To download, use the email and password to login ICPSR.

```{r, eval=FALSE}
# download data from ICPSR to local directory
library(icpsrdata)
icpsr_download(file_id = 37516)
```

```{r}
# import downloaded data
load(file='icpsr_data/37516-0001-Data.rda')
raw <- da37516.0001
```

## Data cleaning

```{r}
# Drop missing data, sample size=4,090
data <- raw[complete.cases(raw[ , c("SMOKER_CATEGORY","ALCOHOLFREQUENCY","TRACT_BELOW_POVERTY","TRACT_UNINSURED_OVER_65","TRACT_OWNER_OVER_30","TRACT_UNINSURED_UNDER_18","COUNTY_VIOLENT_CRIME")]),]

# Keep only needed columns
data <- data %>% 
  dplyr::select(starts_with("TRACT_")|starts_with("COUNTY_")|starts_with("AIR_")|all_of(c("AGE","GENDER","RACETHNICITY","EDUC4","MARITAL","EMPLOY","INCOME","REGION9","METRO","HOUSING","HOME_TYPE","SMOKER_CATEGORY","ALCOHOLFREQUENCY","PHYSICIANS_TG_DOC","WALK_SCORE")))


# Impute those missing air quality data with median
data$missing.airdata <- factor(ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), 1, 0),levels = c(1,0),labels = c("Yes","No"))
data$AIR_QUALITY_GOOD_DAYS <- ifelse(is.na(data$AIR_QUALITY_GOOD_DAYS), median(data$AIR_QUALITY_GOOD_DAYS,na.rm = TRUE), data$AIR_QUALITY_GOOD_DAYS)
data$AIR_QUALITY_MEDIAN_AQI <- ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), median(data$AIR_QUALITY_MEDIAN_AQI,na.rm = TRUE), data$AIR_QUALITY_MEDIAN_AQI)
data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT <- ifelse(is.na(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT), median(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT,na.rm = TRUE), data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT)
data$AIR_QUALITY_DAYS_PM2_5<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM2_5), median(data$AIR_QUALITY_DAYS_PM2_5,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM2_5)
data$AIR_QUALITY_DAYS_PM10<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM10), median(data$AIR_QUALITY_DAYS_PM10,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM10)
```


```{r}
# Add dummy variables
for (x in c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOLFREQUENCY","METRO","HOME_TYPE","SMOKER_CATEGORY","REGION9", "missing.airdata")){
  lbls <- levels(data[[x]])
  levels(data[[x]]) <- sub("^\\([0-9]+\\) +(.+$)", "\\1", lbls)
}

dummy <- fastDummies::dummy_cols(data, remove_first_dummy=TRUE)

# Drop original categorical variables to make the final data

data <- dummy[ , !(names(dummy) %in% c("GENDER","RACETHNICITY","MARITAL","EDUC4","EMPLOY","INCOME","HOUSING","ALCOHOLFREQUENCY","METRO","HOME_TYPE",
                                       "SMOKER_CATEGORY","REGION9", "missing.airdata"))]

names(data) <- make.names(names(data), unique=TRUE)

rm(dummy, da37516.0001)

```

```{r}
colnames(data) <- gsub("[-\\ - \\ \\, ]", "_", gsub("[,\\$]", "", gsub("\\+", "_or_more", gsub("___", "_", colnames(data)))))

data1 <- subset(data,select=-c(GENDER_Female, TRACT_OWNER_OCCUPIED_OF_OCCUPIED))
#gender is binary, and TRACT_RENTER_OCCUPIED_OF_OCCUPID + TRACT_OWNER_OCCUPIED_OF_OCCUPIED = 1, so triggered the collinearity in selection
#data1 <- data %>% subset(select=-TRACT_PERCENT_AM_INDIAN_ALONE) 
```


```{r}
set.seed(1)
train <- sample(1:nrow(data),nrow(data)/2)
```


## Linear Regression

```{r}
#SUBSET SELECTION
#identifying a subset of the predictors we believe to be related to response, fit a model using least squares on the reduced set of variables
#not using best subset due to number of variables, possible models to fit would be huge (2^107)
#perhaps hybrid approach of forward and backward stepwise, as it mimics best subset while retaining computational advantages of forward/backward stepwise
#but am running into issues with multicollinearlity? 

#testing stuff
set.seed(1)
train1 <- sample(1:nrow(data1),nrow(data)/2)


fwd.fit <- regsubsets(WALK_SCORE~., data=data1[train1,], method="forward")
bwd.fit <- regsubsets(WALK_SCORE~., data=data1[train1,], method="backward")
hybrid.fit <- regsubsets(WALK_SCORE~., data=data1[train1,],method="seqrep")

#finding pairs with high correlation
data1.numeric <- model.matrix(~ . - 1, data = data1)
var.cor <- cor(data1_numeric)
which(abs(var.cor) >= 0.95&var.cor!= 1, arr.ind = TRUE)
which(abs(var.cor) >= 0.90&var.cor!=1, arr.ind = TRUE)

```

```{r}
#SHRINKAGE methods, fitting a model involving all predictors, estimated coefficients are shrunken towards zero relative to the least squares estimate, can also perform variable selection. 
#ridge:L2, lasso: L1

#ridge or LASSO? 
#i dont think we need ridge since it doesn't shrink variables to 0? but maybe we could talk about the variables that are close to 0? 

test <- (-train)
x_tr <- as.matrix(train[,-47])
y_tr <- train[,47, drop=TRUE]
x_te <- as.matrix(te_dat[, -47])
y_te <- te_dat[,47, drop=TRUE]

#ridge
#cross validation to choose tuning parameter lambda
# set.seed(1)
# ridge.cv.out <- cv.glmnet(x_tr, y_tr, alpha=0)
# plot(ridge.cv.out)
# ridge.lam <- cv.out$lambda.min 
# 
# ridge.lr <- glmnet(x_tr, y_tr, alpha=0, lambda=ridge.lam, thresh=1e-12)
# ridge.tr.pred <- predict(ridge.lr, newx=x_tr)
# ridge.te.pred <- predict(ridge.lr, newx=x_te)
# ridge.tr.err <- mean((ridge.tr.pred-y_tr)^2)
# ridge.te.err <- mean((ridge.te.pred-y_tr)^2)
# ridge.tr.err
# ridge.te.err
# 
# x <- as.matrix(data[,-47])
# ridge.out <- glmnet(x, data$WALK_SCORE, alpha=0)
# ridge.coef <- predict(ridge.out, type="coefficients", s=ridge.lam)
# ridge.coef[ridge.coef!=0]

#LASSO
set.seed(1)
x <- model.matrix(WALK_SCORE~., )[,]
y <- 
#cross vadliation to choose tuning parameter lambda
lasso.cv.out <- cv.glmnet(x_tr, y_tr, alpha=1)
plot(lasso.cv.out)
lasso.lam <- cv.out$lambda.min 

lasso.lr <- glmnet(x_tr, y_tr, alpha=1, lambda=lasso.lam, thresh=1e-12)
lasso.tr.pred <- predict(lasso.lr, newx=x_tr)
lasso.te.pred <- predict(lasso.lr, newx=x_te)
lasso.tr.err <- mean((lasso.tr.pred-y_tr)^2)
lasso.te.err <- mean((lasso.te.pred-y_tr)^2)
lasso.tr.err
lasso.te.err 

lasso.out <- glmnet(x, data$WALK_SCORE, alpha=1)
lasso.coef <- predict(out, type="coefficients", s=lam)
lasso.coef
lasso.coef[lasso.coef!=0] #79 variables chosen

```



## KNN for regression

```{r}
std <- preProcess(data[train,], method = c("center","scale"))
train_std <- predict(std, newdata = data[train,])
test_std <- predict(std, newdata = data[-train,])

```


## Random Forest
```{r}
rf <- randomForest(WALK_SCORE ~ ., data = data[train,])
yhat.rf <- predict(rf, newdata = data[-train,])
mean((yhat.rf-data[-train,"WALK_SCORE"])^2)

varImpPlot(rf)
```

## Boosting

```{r}
boost <- gbm(WALK_SCORE ~ .,data = data[train,], distribution = "gaussian", n.trees = 30000, interaction.depth = 1, cv.folds = 5, shrinkage = 0.01)
best_n_tress <- which.min(boost$cv.error)

yhat.boost <- predict(boost, newdata = data[-train,], n.trees = best_n_tress)
mean((yhat.boost - data[-train,"WALK_SCORE"])^2)
```

