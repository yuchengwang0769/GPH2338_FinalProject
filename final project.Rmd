---
title: "Final Project for GPH 2338 Machine Learning in Publich Health"
author: "Olivia Chien and Yucheng Wang"
date: "2024-03-25"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
library(leaps)
library(glmnet)
library(caret)
library(plotmo)
```

## Data download

Since ICPSR restricts redistribute the data, a proper way is to download the data to a local directory, for example, the project directory. Only codes and outputs should be push to Github. The data should be kept in the local. To download, use the email and password to login ICPSR.

```{r, eval=FALSE}
# download data from ICPSR to local directory
library(icpsrdata)
icpsr_download(file_id = 37516)
```

```{r}
# import downloaded data
load(file='icpsr_data/37516-0001-Data.rda')
raw <- da37516.0001
```

## Data cleaning

```{r}
# Drop missing data, sample size=4,090
data <- raw[complete.cases(raw[ , c("SMOKER_CATEGORY","ALCOHOL","TRACT_BELOW_POVERTY","TRACT_UNINSURED_OVER_65","TRACT_OWNER_OVER_30","TRACT_UNINSURED_UNDER_18","COUNTY_VIOLENT_CRIME")]),]

# Keep only needed columns
data <- data %>% 
  dplyr::select(starts_with("TRACT_")|starts_with("COUNTY_")|starts_with("AIR_")|all_of(c("AGE","GENDER","RACETHNICITY","EDUC","MARITAL","EMPLOY","INCOME","REGION9","METRO","HOUSING","HOME_TYPE","SMOKER_CATEGORY","ALCOHOL","PHYSICIANS_TG_DOC","WALK_SCORE")))


# Impute those missing air quality data with median
data$missing.airdata <- factor(ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), 1, 0),levels = c(1,0),labels = c("Yes","No"))
data$AIR_QUALITY_GOOD_DAYS <- ifelse(is.na(data$AIR_QUALITY_GOOD_DAYS), median(data$AIR_QUALITY_GOOD_DAYS,na.rm = TRUE), data$AIR_QUALITY_GOOD_DAYS)
data$AIR_QUALITY_MEDIAN_AQI <- ifelse(is.na(data$AIR_QUALITY_MEDIAN_AQI), median(data$AIR_QUALITY_MEDIAN_AQI,na.rm = TRUE), data$AIR_QUALITY_MEDIAN_AQI)
data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT <- ifelse(is.na(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT), median(data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT,na.rm = TRUE), data$AIR_QUALITY_UNHEALTHY_FOR_SENSIT)
data$AIR_QUALITY_DAYS_PM2_5<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM2_5), median(data$AIR_QUALITY_DAYS_PM2_5,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM2_5)
data$AIR_QUALITY_DAYS_PM10<- ifelse(is.na(data$AIR_QUALITY_DAYS_PM10), median(data$AIR_QUALITY_DAYS_PM10,na.rm = TRUE), data$AIR_QUALITY_DAYS_PM10)
```



```{r}
# Add dummy variables
for (x in c("GENDER","RACETHNICITY","MARITAL","EDUC","EMPLOY","INCOME","HOUSING","ALCOHOL","METRO","HOME_TYPE","SMOKER_CATEGORY","REGION9", "missing.airdata")){
  lbls <- levels(data[[x]])
  levels(data[[x]]) <- sub("^\\([0-9]+\\) +(.+$)", "\\1", lbls)
}

dummy <- fastDummies::dummy_cols(data, remove_first_dummy=TRUE)

# Drop original categorical variables to make the final data

data <- dummy[ , !(names(dummy) %in% c("GENDER","RACETHNICITY","MARITAL","EDUC","EMPLOY","INCOME","HOUSING","ALCOHOL","METRO","HOME_TYPE","SMOKER_CATEGORY","REGION9","missing.airdata"))]

names(data) <- make.names(names(data), unique=TRUE)

rm(dummy, da37516.0001)

```

```{r}
set.seed(1)
train <- sample(1:nrow(data),nrow(data)/2)

```


## Linear Regression

```{r}
#SUBSET SELECTION
#identifying a subset of the predictors we believe to be related to response, fit a model using least squares on the reduced set of variables

#not using best subset due to number of variables, possible models to fit would be huge (2^113)

#perhaps hybrid approach of forward and backward stepwise, as it mimics best subset while retaining computational advantages of forward/backward stepwise

#but am running into issues with multicollinearlity? 
## 

#edit dummy

#stepwise selection
fwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="forward", nvmax=113)
best_ind <- which.min(summary(fwd.fit)$bic) #best index using bic
best_coef <- coef(fwd.fit, best_ind)


bwd.fit <- regsubsets(WALK_SCORE~., data=data[train,], method="backward")

hybrid.fit <- regsubsets(WALK_SCORE~., data=data[train,],method="seqrep")

```

```{r}
#SHRINKAGE methods, fitting a model involving all predictors, estimated coefficients are shrunken towards zero relative to the least squares estimate, can also perform variable selection. 
#ridge:L2, lasso: L1

#ridge or LASSO? 
#i dont think we need ridge since it doesn't shrink variables to 0? but maybe we could talk about the variables that are close to 0? 

x_tr <- as.matrix(subset(data[train,],select=-c(WALK_SCORE)))
y_tr <- data[train,"WALK_SCORE"]
x_te <- as.matrix(subset(data[-train,],select=-c(WALK_SCORE)))
y_te <- data[-train,"WALK_SCORE"]
std_fit <- preProcess(x_tr, method = c("center", "scale"))
x_tr_std <- predict(std_fit, x_tr)
x_te_std <- predict(std_fit, x_te)

#ridge
#cross validation to choose tuning parameter lambda
set.seed(1)
ridge.cv.out <- cv.glmnet(x_tr, y_tr, alpha=0, lambda=10^seq(2, -2, by = -.1), nfolds = 5)
plot(ridge.cv.out)
ridge.lam <- ridge.cv.out$lambda.min 
ridge_coef.mt <- predict(ridge.cv.out, s = ridge.lam, type = "coefficients")

ridge.lr <- glmnet(x_tr_std, y_tr, alpha=0, lambda=ridge.lam, thresh=1e-12)
plot(ridge.lr)
ridge_coef <- predict(ridge.lr, s = ridge.lam, type = "coefficients")
ridge.te.pred <- predict(ridge.lr, newx=x_te_std)
ridge.te.err <- mean((ridge.te.pred-y_te)^2)
ridge.te.err

#LASSO
set.seed(1)
lasso.cv.out <- cv.glmnet(x_tr, y_tr, alpha=1, lambda=10^seq(2, -2, by = -.1), nfolds = 5)
plot(lasso.cv.out)
lasso.lam <- lasso.cv.out$lambda.min 

lasso.lr <- glmnet(x_tr_std, y_tr, alpha=1, lambda=lasso.lam, thresh=1e-12)
lasso_coef <- predict(lasso.lr, s = lasso.lam, type = "coefficients")
lasso.te.pred <- predict(lasso.lr, newx=x_te_std)
lasso.te.err <- mean((lasso.te.pred-y_te)^2)
lasso.te.err 
plot_glmnet(lasso.lr)
```



## KNN for regression

```{r}
std <- preProcess(data[train,], method = c("center","scale"))
train_std <- predict(std, newdata = data[train,])
test_std <- predict(std, newdata = data[-train,])

set.seed(1)
fold_ind <- sample(1:5, nrow(train_std), replace = TRUE)

knn_by_k <-  function(k) {
    cv.error <- mean(sapply(1:5, function(j){
    fit <- knnreg(WALK_SCORE ~ ., data = train_std[fold_ind != j, ], k = k)
    pred <- predict(fit, newdata = train_std[fold_ind == j, ])
    mean((train_std$WALK_SCORE[fold_ind == j] - pred)^2)
    }))
    data.frame(k=k, cv.error=cv.error)
  }
knn_cv <- do.call(rbind,lapply(1:100,knn_by_k))
plot(knn_cv$k,knn_cv$cv.error)
best_k <- knn_cv$k[knn_cv$cv.error==min(knn_cv$cv.error)]

knn.final <- knnreg(WALK_SCORE ~ ., data = train_std, k = k)

```


## Random Forest
```{r}
rf <- randomForest(WALK_SCORE ~ ., data = data[train,])
yhat.rf <- predict(rf, newdata = data[-train,])
mean((yhat.rf-data[-train,"WALK_SCORE"])^2)

varImpPlot(rf)
```

## Boosting

```{r}
boost <- gbm(WALK_SCORE ~ .,data = data[train,], distribution = "gaussian", n.trees = 30000, interaction.depth = 1, cv.folds = 5, shrinkage = 0.01)
best_n_tress <- which.min(boost$cv.error)

yhat.boost <- predict(boost, newdata = data[-train,], n.trees = best_n_tress)
mean((yhat.boost - data[-train,"WALK_SCORE"])^2)
```

